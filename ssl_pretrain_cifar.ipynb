{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfbf324a-50b7-4c41-982e-a2e8203e6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "MNIST SimCLR vs Supervised — Noisy-Label Robustness Experiment (PyTorch)\n",
    "\n",
    "Single-file, reproducible pipeline to:\n",
    "  1) Train a baseline supervised classifier on MNIST (clean or noisy labels)\n",
    "  2) Pretrain with SimCLR on MNIST (no labels), then fine-tune supervised\n",
    "  3) Compare results side-by-side and save metrics to CSV\n",
    "\n",
    "Usage examples\n",
    "--------------\n",
    "# 1) Baseline supervised on clean MNIST\n",
    "python mnist_ssl_noise_experiment.py --device cuda --mode baseline --epochs 10\n",
    "\n",
    "# 2) SimCLR pretrain + fine-tune on clean MNIST\n",
    "python mnist_ssl_noise_experiment.py --device cuda --mode simclr_then_finetune --pretrain-epochs 50 --epochs 10\n",
    "\n",
    "# 3) Baseline supervised with 40% symmetric label noise on train set\n",
    "python mnist_ssl_noise_experiment.py --device cuda --mode baseline --noise-rate 0.40 --epochs 10\n",
    "\n",
    "# 4) SimCLR pretrain + fine-tune with 40% label-noisy training set\n",
    "python mnist_ssl_noise_experiment.py --device cuda \\\n",
    "  --mode simclr_then_finetune --noise-rate 0.40 --pretrain-epochs 50 --epochs 10\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "- runs/metrics.csv: one row per run with settings and test accuracy\n",
    "- runs/checkpoints/: model checkpoints\n",
    "- runs/logs.txt: basic text log\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- SimCLR uses unlabeled views of the *same* MNIST images (train split), independent\n",
    "  of any label corruption you may introduce for the downstream fine-tuning.\n",
    "- We slightly tweak ResNet-18 for 1-channel 28x28 images.\n",
    "- Augmentations are adapted for grayscale & small images.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, Optional\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Utilities\n",
    "# --------------------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Data\n",
    "# --------------------\n",
    "\n",
    "class NoisyLabelWrapper(Dataset):\n",
    "    \"\"\"Wrap a torchvision dataset to inject symmetric label noise on-the-fly.\n",
    "\n",
    "    The noisy labels are generated once at init for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, base: Dataset, noise_rate: float, num_classes: int, seed: int = 42):\n",
    "        self.base = base\n",
    "        self.noise_rate = noise_rate\n",
    "        self.num_classes = num_classes\n",
    "        g = random.Random(seed)\n",
    "        # precompute noisy labels\n",
    "        self.targets = []\n",
    "        for _, y in base:\n",
    "            if g.random() < noise_rate:\n",
    "                ny = g.randrange(num_classes)\n",
    "                if ny == y:\n",
    "                    ny = (ny + 1) % num_classes\n",
    "                self.targets.append(ny)\n",
    "            else:\n",
    "                self.targets.append(y)\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, _ = self.base[i]\n",
    "        return x, self.targets[i]\n",
    "\n",
    "\n",
    "\n",
    "class SimCLRAugment:\n",
    "    \"\"\"Augmentations for SimCLR adapted to MNIST (1x28x28).\n",
    "\n",
    "    We resize to 32x32 for a more comfortable ReNet-18 receptive field.\n",
    "    \"\"\"\n",
    "    def __init__(self, size: int = 32):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.RandomResizedCrop(size=size, scale=(0.6, 1.0)),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xi = self.transform(x)\n",
    "        xj = self.transform(x)\n",
    "        return xi, xj\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size: int, noise_rate: float, num_workers: int,\n",
    "                    seed: int, for_simclr: bool, dataset: str):\n",
    "    \"\"\"\n",
    "    Returns train_loader, test_loader, ssl_dataset (if for_simclr=True).\n",
    "    \"\"\"\n",
    "    if dataset.lower() in ['cifar10', 'cifar100']:\n",
    "        mean, std, num_classes = get_cifar_norm(dataset)\n",
    "        sup_train_tf = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        test_tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        root = os.path.join(\"data\", dataset.lower())\n",
    "\n",
    "        if dataset.lower() == 'cifar10':\n",
    "            base_train = datasets.CIFAR10(root=root, train=True, download=True, transform=sup_train_tf)\n",
    "            test = datasets.CIFAR10(root=root, train=False, download=True, transform=test_tf)\n",
    "        else:\n",
    "            base_train = datasets.CIFAR100(root=root, train=True, download=True, transform=sup_train_tf)\n",
    "            test = datasets.CIFAR100(root=root, train=False, download=True, transform=test_tf)\n",
    "\n",
    "        train = NoisyLabelWrapper(base_train, noise_rate=noise_rate, num_classes=num_classes, seed=seed) \\\n",
    "                if noise_rate > 0 else base_train\n",
    "\n",
    "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,\n",
    "                                  num_workers=num_workers, pin_memory=True)\n",
    "        test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                                 num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        ssl_ds = None\n",
    "        if for_simclr:\n",
    "            # Important: SimCLR pipeline should NOT include normalization\n",
    "            tf = SimCLRAugmentCIFAR(size=32)\n",
    "            if dataset.lower() == 'cifar10':\n",
    "                ssl_ds = datasets.CIFAR10(root=root, train=True, download=True, transform=tf)\n",
    "            else:\n",
    "                ssl_ds = datasets.CIFAR100(root=root, train=True, download=True, transform=tf)\n",
    "\n",
    "        return train_loader, test_loader, ssl_ds\n",
    "\n",
    "    elif dataset.lower() == 'mnist':\n",
    "        # keep your existing MNIST path if you still want MNIST runs\n",
    "        ...\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Models\n",
    "# --------------------\n",
    "class ResNet18Small(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-18 adapted for small images.\n",
    "    - conv1: kernel=3, stride=1, padding=1\n",
    "    - remove first maxpool\n",
    "    - in_channels selectable (1 for MNIST, 3 for CIFAR)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 10, in_channels: int = 3):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(weights=None)\n",
    "        base.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        base.maxpool = nn.Identity()\n",
    "        self.features = nn.Sequential(*list(base.children())[:-1])  # global avgpool yields [B,512,1,1]\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x, return_feat=False):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if return_feat:\n",
    "            return x\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int = 512, hidden_dim: int = 512, out_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.ReLU(inplace=True), nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder: ResNet18Small, proj: ProjectionHead):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.proj = proj\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x, return_feat=True)\n",
    "        z = self.proj(feat)\n",
    "        return z\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Losses\n",
    "# --------------------\n",
    "class NTXent(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.t = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        # z_i, z_j: [B, D] normalized\n",
    "        B, D = z_i.shape\n",
    "        z = torch.cat([z_i, z_j], dim=0)  # [2B, D]\n",
    "        sim = torch.matmul(z, z.T)  # cosine sim since normalized\n",
    "        # mask out self-similarity\n",
    "        mask = torch.eye(2 * B, dtype=torch.bool, device=z.device)\n",
    "        sim = sim.masked_fill(mask, -9e15)\n",
    "        # positives: for each i in [0..B-1], positive is i+B; for i in [B..2B-1], positive is i-B\n",
    "        pos = torch.cat([torch.arange(B, 2 * B), torch.arange(0, B)]).to(z.device)\n",
    "        logits = sim / self.t\n",
    "        labels = pos\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Training / Evaluation\n",
    "# --------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct, total, loss_accum = 0, 0, 0.0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = ce(logits, y)\n",
    "        loss_accum += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return correct / total, loss_accum / total\n",
    "\n",
    "\n",
    "def train_supervised(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader,\n",
    "                     device: str, epochs: int = 10, lr: float = 1e-3, wd: float = 1e-4,\n",
    "                     save_path: Optional[str] = None):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    best_acc = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = ce(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        acc, tloss = evaluate(model, test_loader, device)\n",
    "        print(f\"[Supervised] Epoch {ep:03d} | Test Acc={acc*100:.2f}% | Test Loss={tloss:.4f}\")\n",
    "        if save_path and acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "def pretrain_simclr(ssl_model: SimCLR, ssl_loader: DataLoader, device: str, epochs: int = 50,\n",
    "                    lr: float = 1e-3, wd: float = 1e-4, temp: float = 0.5):\n",
    "    loss_fn = NTXent(temperature=temp)\n",
    "    opt = torch.optim.Adam(ssl_model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ssl_model.train()\n",
    "    for ep in range(1, epochs + 1):\n",
    "        loss_accum, n = 0.0, 0\n",
    "        for (x_i, x_j), _ in ssl_loader:\n",
    "            x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "            opt.zero_grad()\n",
    "            z_i = ssl_model(x_i)\n",
    "            z_j = ssl_model(x_j)\n",
    "            loss = loss_fn(z_i, z_j)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            bs = x_i.size(0)\n",
    "            loss_accum += loss.item() * bs\n",
    "            n += bs\n",
    "        print(f\"[SimCLR] Epoch {ep:03d} | Loss={loss_accum / max(n,1):.4f}\")\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Orchestration\n",
    "# --------------------\n",
    "@dataclass\n",
    "class RunResult:\n",
    "    mode: str\n",
    "    noise_rate: float\n",
    "    pretrain_epochs: int\n",
    "    finetune_epochs: int\n",
    "    test_acc: float\n",
    "    timestamp: float\n",
    "\n",
    "\n",
    "def save_metrics_row(path_csv: str, row: RunResult):\n",
    "    header = list(asdict(row).keys())\n",
    "    exists = os.path.exists(path_csv)\n",
    "    with open(path_csv, 'a', newline='') as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        if not exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(asdict(row))\n",
    "\n",
    "\n",
    "\n",
    "def build_ssl_loader(batch_size: int, num_workers: int, dataset: str) -> DataLoader:\n",
    "  #CIFAR version\n",
    "    if dataset.lower() in ['cifar10', 'cifar100']:\n",
    "        tf = SimCLRAugmentCIFAR(size=32)\n",
    "        root = os.path.join(\"data\", dataset.lower())\n",
    "        base = datasets.CIFAR10 if dataset.lower()=='cifar10' else datasets.CIFAR100\n",
    "        ds = base(root=root, train=True, download=True, transform=tf)\n",
    "\n",
    "        class _Wrap(Dataset):\n",
    "            def __init__(self, ds): self.ds = ds\n",
    "            def __len__(self): return len(self.ds)\n",
    "            def __getitem__(self, idx):\n",
    "                (xi, xj), _ = self.ds[idx]\n",
    "                return (xi, xj), 0\n",
    "\n",
    "        ssl = _Wrap(ds)\n",
    "        return DataLoader(ssl, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "    else:\n",
    "        #MNIST version\n",
    "      tf = SimCLRAugment(size=32)\n",
    "      ssl_train = datasets.MNIST(root=os.path.join(\"data\",\"mnist\"), train=True, download=True, transform=tf)\n",
    "      # The transform already returns two views, but torchvision MNIST expects a single tensor.\n",
    "      # Wrap to conform to ((xi,xj), _)\n",
    "      class _Wrap(Dataset):\n",
    "          def __init__(self, ds): self.ds = ds\n",
    "          def __len__(self): return len(self.ds)\n",
    "          def __getitem__(self, idx):\n",
    "              (xi, xj), _ = self.ds[idx]\n",
    "              return (xi, xj), 0\n",
    "      ssl_train = _Wrap(ssl_train)\n",
    "      ssl_loader = DataLoader(ssl_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "      return ssl_loader\n",
    "\n",
    "def save_encoder(encoder: nn.Module, path: str):\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    torch.save({'features': encoder.features.state_dict()}, path)\n",
    "\n",
    "def load_encoder_into_classifier(path: str, clf: nn.Module):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    state = ckpt.get('features', ckpt)  # backward-compat if raw dict saved\n",
    "    missing, unexpected = clf.features.load_state_dict(state, strict=False)\n",
    "    print(f\"Loaded encoder from {path}. Missing={missing}, Unexpected={unexpected}\")\n",
    "    return clf\n",
    "\n",
    "\n",
    "def run_ex(args):\n",
    "    \"\"\"Extended runner with reusable pretrained encoder support.\n",
    "    Modes: 'baseline', 'simclr_then_finetune', 'finetune_from_pretrained'\n",
    "    \"\"\"\n",
    "    set_seed(args.seed)\n",
    "    device = args.device if torch.cuda.is_available() and args.device.startswith('cuda') else 'cpu'\n",
    "    ensure_dir('runs'); ensure_dir('runs/checkpoints')\n",
    "\n",
    "    num_classes = 100 if args.dataset.lower()=='cifar100' else 10\n",
    "    in_ch = 3  # CIFAR is RGB\n",
    "\n",
    "\n",
    "    # Supervised data (label noise applied here only)\n",
    "    train_loader, test_loader, _ = get_dataloaders(\n",
    "        batch_size=args.batch_size,\n",
    "        noise_rate=args.noise_rate,\n",
    "        num_workers=args.workers,\n",
    "        seed=args.seed,\n",
    "        for_simclr=False,\n",
    "        dataset=args.dataset\n",
    "    )\n",
    "\n",
    "    if args.mode == 'baseline':\n",
    "        model = ResNet18Small(num_classes=10).to(device)\n",
    "        ckpt = os.path.join('runs', 'checkpoints', f\"baseline_noise{args.noise_rate:.2f}.pt\")\n",
    "        acc = train_supervised(model, train_loader, test_loader, device,\n",
    "                               epochs=args.epochs, lr=args.lr, wd=args.wd,\n",
    "                               save_path=ckpt)\n",
    "        print(f\"Final Test Acc (baseline, noise={args.noise_rate:.2f}): {acc*100:.2f}%\")\n",
    "        save_metrics_row(os.path.join('runs','metrics.csv'), RunResult(\n",
    "            mode='baseline', noise_rate=args.noise_rate,\n",
    "            pretrain_epochs=0, finetune_epochs=args.epochs,\n",
    "            test_acc=acc, timestamp=time.time()\n",
    "        ))\n",
    "        return\n",
    "\n",
    "    if args.mode == 'simclr_then_finetune':\n",
    "        # Pretrain on unlabeled data\n",
    "        ssl_loader = build_ssl_loader(args.batch_size_ssl, args.workers, args.dataset)\n",
    "        encoder = ResNet18Small(num_classes=num_classes, in_channels=in_ch).to(device)\n",
    "        proj = ProjectionHead(in_dim=512, hidden_dim=512, out_dim=args.proj_dim).to(device)\n",
    "        ssl_model = SimCLR(encoder, proj).to(device)\n",
    "\n",
    "        pretrain_simclr(ssl_model, ssl_loader, device,\n",
    "                        epochs=args.pretrain_epochs, lr=args.pretrain_lr,\n",
    "                        wd=args.pretrain_wd, temp=args.temperature)\n",
    "        # Save encoder for reuse\n",
    "        if getattr(args, 'save_pretrained_encoder_path', ''):\n",
    "            save_encoder(encoder, args.save_pretrained_encoder_path)\n",
    "            print(f\"Saved pretrained encoder to: {args.save_pretrained_encoder_path}\")\n",
    "        # Fine-tune\n",
    "        clf = ResNet18Small(num_classes=num_classes, in_channels=in_ch).to(device)\n",
    "        clf.features.load_state_dict(encoder.features.state_dict())\n",
    "        if getattr(args, 'reset_classifier_head', True):\n",
    "            clf.fc.reset_parameters()\n",
    "        if args.freeze_backbone:\n",
    "            for p in clf.features.parameters():\n",
    "                p.requires_grad = False\n",
    "        ckpt = os.path.join('runs', 'checkpoints',\n",
    "                            f\"simclr_noise{args.noise_rate:.2f}_freeze{int(args.freeze_backbone)}.pt\")\n",
    "        acc = train_supervised(clf, train_loader, test_loader, device,\n",
    "                               epochs=args.epochs, lr=args.lr, wd=args.wd,\n",
    "                               save_path=ckpt)\n",
    "        print(f\"Final Test Acc (SimCLR→FT, noise={args.noise_rate:.2f}, freeze={args.freeze_backbone}): {acc*100:.2f}%\")\n",
    "        save_metrics_row(os.path.join('runs','metrics.csv'), RunResult(\n",
    "            mode=f\"simclr_then_finetune_freeze{int(args.freeze_backbone)}\", noise_rate=args.noise_rate,\n",
    "            pretrain_epochs=args.pretrain_epochs, finetune_epochs=args.epochs,\n",
    "            test_acc=acc, timestamp=time.time()\n",
    "        ))\n",
    "        return\n",
    "\n",
    "    if args.mode == 'finetune_from_pretrained':\n",
    "        if not getattr(args, 'pretrained_encoder_path', '') or not os.path.exists(args.pretrained_encoder_path):\n",
    "            raise FileNotFoundError(\"Set args.pretrained_encoder_path to a valid encoder .pt saved from SimCLR pretrain.\")\n",
    "        clf = ResNet18Small(num_classes=num_classes, in_channels=in_ch).to(device)\n",
    "        load_encoder_into_classifier(args.pretrained_encoder_path, clf)\n",
    "        if getattr(args, 'reset_classifier_head', True):\n",
    "            clf.fc.reset_parameters()\n",
    "        if args.freeze_backbone:\n",
    "            for p in clf.features.parameters():\n",
    "                p.requires_grad = False\n",
    "        ckpt = os.path.join('runs', 'checkpoints',\n",
    "                            f\"ft_from_pretrained_noise{args.noise_rate:.2f}_freeze{int(args.freeze_backbone)}.pt\")\n",
    "        acc = train_supervised(clf, train_loader, test_loader, device,\n",
    "                               epochs=args.epochs, lr=args.lr, wd=args.wd,\n",
    "                               save_path=ckpt)\n",
    "        print(f\"Final Test Acc (FT from pretrained, noise={args.noise_rate:.2f}, freeze={args.freeze_backbone}): {acc*100:.2f}%\")\n",
    "        save_metrics_row(os.path.join('runs','metrics.csv'), RunResult(\n",
    "            mode=f\"finetune_from_pretrained_freeze{int(args.freeze_backbone)}\", noise_rate=args.noise_rate,\n",
    "            pretrain_epochs=0, finetune_epochs=args.epochs,\n",
    "            test_acc=acc, timestamp=time.time()\n",
    "        ))\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown mode: {args.mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf4cdd2-f058-44f2-8621-aa58176d7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "CIFAR10_MEAN, CIFAR10_STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "CIFAR100_MEAN, CIFAR100_STD = (0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "def get_cifar_norm(dataset):\n",
    "    if dataset.lower() == 'cifar10':\n",
    "        return CIFAR10_MEAN, CIFAR10_STD, 10\n",
    "    elif dataset.lower() == 'cifar100':\n",
    "        return CIFAR100_MEAN, CIFAR100_STD, 100\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'cifar10' or 'cifar100'\")\n",
    "\n",
    "class SimCLRAugmentCIFAR:\n",
    "    \"\"\"SimCLR-style augs for 32x32 color images.\"\"\"\n",
    "    def __init__(self, size=32):\n",
    "        self.base = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    def __call__(self, img):\n",
    "        return self.base(img), self.base(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6343f8e5-ed20-4bdc-a6a7-0aa3fe9a2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    # Core run switches\n",
    "    mode='baseline',                  # 'baseline', 'simclr_then_finetune', 'finetune_from_pretrained'\n",
    "    dataset='cifar10',                # 'cifar10' or 'cifar100' (keep 'mnist' if you want both)\n",
    "    device='cuda',\n",
    "    seed=42,\n",
    "    batch_size=256,\n",
    "    workers=1,\n",
    "    #epochs=50,                        # CIFAR usually benefits from more epochs than MNIST\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    wd=1e-4,\n",
    "    noise_rate=0.0,                   # symmetric label noise on train set\n",
    "\n",
    "    # SSL pretrain\n",
    "    #pretrain_epochs=200,              # SimCLR typically needs longer on CIFAR\n",
    "    pretrain_epochs=10,\n",
    "    pretrain_lr=1e-3,\n",
    "    pretrain_wd=1e-4,\n",
    "    batch_size_ssl=256,\n",
    "    proj_dim=128,\n",
    "    temperature=0.5,\n",
    "    freeze_backbone=False,\n",
    "\n",
    "    # Reuse / checkpointing\n",
    "    save_pretrained_encoder_path='runs/checkpoints/simclr_cifar_encoder.pt',\n",
    "    pretrained_encoder_path='',\n",
    "    reset_classifier_head=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3223482-bf22-4f14-8b15-a66ab9921af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nividia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nividia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319144d-dc44-497d-9a76-e34c97813fed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "args.mode = 'baseline'\n",
    "args.noise_rate = 0.0\n",
    "run_ex(args)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Duration: {(end - start)/60:.2f} min ({end - start:.1f} sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238507fb-573c-4ea1-8d42-bc3b5dac6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "args.dataset = 'cifar10'\n",
    "args.mode = 'simclr_then_finetune'\n",
    "args.noise_rate = 0.0             # irrelevant to pretrain; used for the FT in this step\n",
    "args.pretrain_epochs = 10\n",
    "args.save_pretrained_encoder_path = 'runs/checkpoints/simclr_cifar10_encoder_e10.pt'\n",
    "run_ex(args)                       # (or your run(args) if you merged)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Duration: {(end - start)/60:.2f} min ({end - start:.1f} sec)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
